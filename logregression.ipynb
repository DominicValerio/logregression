{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5f6f9e",
   "metadata": {},
   "source": [
    "# Supervised Learning: Logistic Regression\n",
    "\n",
    "Prediction Function\n",
    "$$\\hat{y} = \\sigma(z)$$\n",
    "\n",
    "Activation Function - Sigmoid\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "- Turns the logit's linear score into a probability \n",
    "\n",
    "Score function - Logit\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\n",
    "  &\\mathbf{w}\\text{: weight vector}\\\\\n",
    "  &\\mathbf{x}\\text{: input feature vector}\\\\\n",
    "  &b\\text{: bias term}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- A linear combination of the weights and features plus bias\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model parameters\n",
    "import numpy as np\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 1000\n",
    "\n",
    "def prediction_function(z):\n",
    "    return (1) / (1+np.exp(-z))\n",
    "\n",
    "def score_function(w, x, b):\n",
    "    return np.dot(x, w) + b\n",
    "\n",
    "weights = []\n",
    "bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9491fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the training data\n",
    "\n",
    "# [kills, deaths]\n",
    "features = [[3, 6], [7,7], [2,4], [3,5], [0,5], [4, 5], [2, 4], [0, 4]] \n",
    "# [win/loss]\n",
    "labels = [1, 1, 1, 0, 1, 1, 1, 0]\n",
    "\n",
    "# convert to numpy arrays for matrix and vector operations\n",
    "features = np.array(features); labels = np.array(labels); weights = np.array(np.zeros(features.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329b21a",
   "metadata": {},
   "source": [
    "Loss Function - Cross-Entropy\n",
    "$$\\mathcal{L}(y, \\hat{y}) = -\\left[ y \\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}) \\right]$$\n",
    "\n",
    "- Loss is a number indicating how\n",
    "bad the model's prediction was on a\n",
    "single example. If the model's\n",
    "prediction is perfect, the loss is zero;\n",
    "otherwise, the loss is greater.\n",
    "\n",
    "Cost Function\n",
    "\n",
    "$$J(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)})\\log(1 - \\hat{y}^{(i)}) \\right]$$\n",
    "\n",
    "- The cost function is the average loss across all samples. The ultimate goal is to minimize the cost of the model.  \n",
    "\n",
    "In Gradient Descent we want to move in the opposite direction of where Loss increases the most each iteration.\n",
    " \n",
    "$$\\mathcal{L}(y, \\hat{y})= \\mathcal{L}(y, f(\\mathbf{w}, \\mathbf{x}, b))$$\n",
    "\n",
    "$ y $ and $\\mathbf{x}$ are both constant to this function so you just need\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "&\\nabla L = \\left( \\frac{\\partial L}{\\partial \\mathbf{w}}, \\frac{\\partial L}{\\partial b} \\right)\\\\\n",
    "&\\frac{\\partial L}{\\partial \\mathbf{w}} = (\\hat{y} - y) \\mathbf{x}, \\space \\frac{\\partial L}{\\partial b} = \\hat{y} - y\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using gradient descent \n",
    "n = len(features)\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    # compute the prediction function\n",
    "    z = score_function(weights, features, bias)\n",
    "    y_hat = prediction_function(z)\n",
    "\n",
    "    # compute gradient components\n",
    "    dL_dw = np.dot(features.T, (y_hat - labels) )\n",
    "    dL_db = y_hat - labels\n",
    "    \n",
    "    dJ_dw = 1/n * dL_dw\n",
    "    dJ_db = 1/n * np.sum(dL_db)\n",
    "\n",
    "    # update the weights and bias\n",
    "    weights += -dJ_dw * LEARNING_RATE\n",
    "    bias += -dJ_db * LEARNING_RATE\n",
    "    # since differentials are the direction where loss increases the most,\n",
    "    # move in the opposite direction of those differentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da86f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: 0.36 0.19 \n",
      "Bias: -0.58\n",
      "\n",
      "X testing data: [[3, 6], [0, 2], [3, 5]]\n",
      "Probability of elements: 0.84 0.45 0.81 \n",
      "Prediction of successes: [1 0 1]\n"
     ]
    }
   ],
   "source": [
    "def predict_probability(X_test):\n",
    "    return prediction_function(score_function(weights, X_test, bias))\n",
    "\n",
    "def predict_successes(X_test):\n",
    "    return np.where(predict_probability(X_test) >= 0.5, 1, 0)\n",
    "\n",
    "# note: The magnitude of the weights can tell you how much a feature affects the prediction\n",
    "print(f\"Weights: \", end=\"\")\n",
    "for v in weights:\n",
    "    print(f\"{v:.2f} \", end=\"\")\n",
    "print()\n",
    "print(f\"Bias: {bias:.2f}\\n\")\n",
    "\n",
    "X_test = [ [3,6], [0, 2], [3,5] ]\n",
    "print(\"X testing data: \", end=\"\"); print(X_test)\n",
    "\n",
    "print(f\"Probability of elements: \", end=\"\")\n",
    "for v in X_test:\n",
    "    print(f\"{predict_probability(v):.2f} \", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"Prediction of successes: {predict_successes(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e201a67",
   "metadata": {},
   "source": [
    "#### Loss function derivation for logistic regression\n",
    "\n",
    "The Bernoulli Distribution\n",
    "\n",
    "$$P(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\}, \\; 0 \\leq p \\leq 1$$\n",
    "\n",
    "The Bernoulli distribution models the outcome of a single trial that has only two possible outcomes:  \n",
    "1: Success  \n",
    "0: Failure  \n",
    "  \n",
    "$p$: the probability of success  \n",
    "\n",
    "Likelihood of the data happening, given parameters for a distribution $P$\n",
    "$$L(\\theta) = \\prod_{i=1}^{n} P(x_i \\mid \\theta)$$\n",
    "\n",
    "The Log-Likelihood is equivalent but faster computationally\n",
    "$$\\log L(\\theta) = \\sum_{i=1}^{n} \\log P(x_i \\mid \\theta)$$\n",
    "\n",
    "Cross-Entropy\n",
    "\n",
    "$$H(P, Q) = -\\sum_{x} P(x) \\log Q(x)$$\n",
    "\n",
    "Cross-Entropy is a measure of the difference between two probability distributions. \n",
    "\n",
    "$P$: True distribution  \n",
    "$Q$: Predicted distribution  \n",
    "\n",
    "In logistic regression, there are only two outcomes so it follows a Bernoulli Distribution. $P$ here would be the Bernoulli Distribution.\n",
    "\n",
    "$$L(p) = \\prod_{i=1}^{n} p^{y_i} (1 - p)^{1 - y_i}$$\n",
    "$$\\log L(p) = \\sum_{i=1}^{n} \\left[ y_i \\log p + (1 - y_i) \\log (1 - p) \\right]$$\n",
    "\n",
    "\n",
    "We can use cross-entropy to measure the loss between the actual probability distribution, and the predicted one with our model. And since we know Cross-Entropy is the negative of the Likelihood function in this case. That brings us to this:\n",
    "\n",
    "$$H(P, Q) = -L(p) = -\\log L(p)$$\n",
    "\n",
    "Loss is equal to Cross-Entropy\n",
    "$$\\mathcal{L}(y, \\hat y)= H(P, Q) = -\\log L(p) = -\\sum_{i=1}^{n} \\left[ y_i \\log p + (1 - y_i) \\log (1 - p) \\right]$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b8298",
   "metadata": {},
   "source": [
    "https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
